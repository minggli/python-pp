{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from typing import List, Optional, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pyro\n",
    "from pyro.distributions import Normal, Uniform, Delta\n",
    "from pyro.infer import SVI, Trace_ELBO\n",
    "from pyro.optim import Adam\n",
    "from pyro.distributions.util import logsumexp\n",
    "from pyro.infer import EmpiricalMarginal, SVI, Trace_ELBO, TracePredictive\n",
    "from pyro.infer.mcmc import MCMC, NUTS\n",
    "import pyro.optim as optim\n",
    "import pyro.poutine as poutine\n",
    "\n",
    "# for CI testing\n",
    "smoke_test = ('CI' in os.environ)\n",
    "# assert pyro.__version__.startswith('0.3.1')\n",
    "pyro.enable_validation(True)\n",
    "pyro.set_rng_seed(1)\n",
    "pyro.enable_validation(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_URL = \"https://d2fefpcigoriu7.cloudfront.net/datasets/rugged_data.csv\"\n",
    "data = pd.read_csv(DATA_URL, encoding=\"ISO-8859-1\")\n",
    "df = data[[\"cont_africa\", \"rugged\", \"rgdppc_2000\"]]\n",
    "df = df[np.isfinite(df.rgdppc_2000)]\n",
    "df[\"rgdppc_2000\"] = np.log(df[\"rgdppc_2000\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 170 entries, 2 to 233\n",
      "Data columns (total 3 columns):\n",
      "cont_africa    170 non-null int64\n",
      "rugged         170 non-null float64\n",
      "rgdppc_2000    170 non-null float64\n",
      "dtypes: float64(2), int64(1)\n",
      "memory usage: 5.3 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f4649e49198>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.distplot(df[\"rugged\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OLS(nn.Module):\n",
    "    def __init__(self, num_params):\n",
    "        super(OLS, self).__init__()\n",
    "        self.linear = nn.Linear(num_params, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OLS(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = torch.rand(1000, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = x_data @ torch.tensor([[2],[-1], [1]], dtype=torch.float32) + torch.tensor(1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/li/.virtualenvs/pp/lib/python3.6/site-packages/torch/nn/modules/loss.py:443: UserWarning: Using a target size (torch.Size([1000, 1])) that is different to the input size (torch.Size([1000])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 0050] loss: 491375.2812\n",
      "[iteration 0100] loss: 490273.1250\n",
      "[iteration 0150] loss: 490260.5000\n",
      "[iteration 0200] loss: 490260.5000\n",
      "[iteration 0250] loss: 490260.5000\n",
      "[iteration 0300] loss: 490260.5000\n",
      "[iteration 0350] loss: 490260.4688\n",
      "[iteration 0400] loss: 490260.4688\n",
      "[iteration 0450] loss: 490260.4688\n",
      "[iteration 0500] loss: 490260.4688\n",
      "[iteration 0550] loss: 490260.4688\n",
      "[iteration 0600] loss: 490260.4688\n",
      "[iteration 0650] loss: 490260.4688\n",
      "[iteration 0700] loss: 490260.4688\n",
      "[iteration 0750] loss: 490260.4688\n",
      "[iteration 0800] loss: 490260.4688\n",
      "[iteration 0850] loss: 490260.4688\n",
      "[iteration 0900] loss: 490260.4688\n",
      "[iteration 0950] loss: 490260.4688\n",
      "[iteration 1000] loss: 490260.4688\n",
      "Learned parameters:\n",
      "linear.weight [[-2.3307880e-08 -2.5608076e-08 -2.4100025e-08]]\n",
      "linear.bias [0.9827749]\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "num_iterations = 1000\n",
    "\n",
    "\n",
    "def main():\n",
    "    for j in range(num_iterations):\n",
    "        # run the model forward on the data\n",
    "        y_pred = model(x_data).squeeze(-1)\n",
    "        # calculate the mse loss\n",
    "        loss = loss_fn(y_pred, y_data)\n",
    "        # initialize gradients to zero\n",
    "        optim.zero_grad()\n",
    "        # backpropagate\n",
    "        loss.backward()\n",
    "        # take a gradient step\n",
    "        optim.step()\n",
    "        if (j + 1) % 50 == 0:\n",
    "            print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss.item()))\n",
    "    # Inspect learned parameters\n",
    "    print(\"Learned parameters:\")\n",
    "    for name, param in model.named_parameters():\n",
    "        print(name, param.data.numpy())\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Bayesian Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(x_data, y_data):\n",
    "    # weight and bias priors\n",
    "    w_prior = Normal(torch.zeros(1, 2), torch.ones(1, 2)).to_event(1)\n",
    "    b_prior = Normal(torch.tensor([[8.]]), torch.tensor([[1000.]])).to_event(1)\n",
    "    f_prior = Normal(0., 1.)\n",
    "    priors = {'linear.weight': w_prior, 'linear.bias': b_prior, 'factor': f_prior}\n",
    "    scale = pyro.sample(\"sigma\", Uniform(0., 10.))\n",
    "    # lift module parameters to random variables sampled from the priors\n",
    "    lifted_module = pyro.random_module(\"module\", regression_model, priors)\n",
    "    # sample a nn (which also samples w and b)\n",
    "    lifted_reg_model = lifted_module()\n",
    "    with pyro.plate(\"map\", len(x_data)):\n",
    "        # run the nn forward on data\n",
    "        prediction_mean = lifted_reg_model(x_data).squeeze(-1)\n",
    "        # condition on the observed data\n",
    "        pyro.sample(\"obs\",\n",
    "                    Normal(prediction_mean, scale),\n",
    "                    obs=y_data)\n",
    "        return prediction_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.contrib.autoguide import AutoDiagonalNormal\n",
    "guide = AutoDiagonalNormal(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = Adam({\"lr\": 0.03})\n",
    "svi = SVI(model, guide, optim, loss=Trace_ELBO(), num_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'regression_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-683f0d7ec27e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[iteration %04d] loss: %.4f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-683f0d7ec27e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m# calculate the loss and take a gradient step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[iteration %04d] loss: %.4f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pp/lib/python3.6/site-packages/pyro/infer/svi.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# get loss and compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mpoutine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mparam_capture\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_and_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         params = set(site[\"value\"].unconstrained()\n",
      "\u001b[0;32m~/.virtualenvs/pp/lib/python3.6/site-packages/pyro/infer/trace_elbo.py\u001b[0m in \u001b[0;36mloss_and_grads\u001b[0;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;31m# grab a trace from the generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmodel_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide_trace\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_traces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m             \u001b[0mloss_particle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msurrogate_loss_particle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_differentiable_loss_particle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_trace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_particle\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_particles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pp/lib/python3.6/site-packages/pyro/infer/elbo.py\u001b[0m in \u001b[0;36m_get_traces\u001b[0;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_particles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/pp/lib/python3.6/site-packages/pyro/infer/trace_elbo.py\u001b[0m in \u001b[0;36m_get_trace\u001b[0;34m(self, model, guide, *args, **kwargs)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \"\"\"\n\u001b[1;32m     51\u001b[0m         model_trace, guide_trace = get_importance_trace(\n\u001b[0;32m---> 52\u001b[0;31m             \"flat\", self.max_plate_nesting, model, guide, *args, **kwargs)\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_validation_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mcheck_if_enumerated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mguide_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pp/lib/python3.6/site-packages/pyro/infer/enum.py\u001b[0m in \u001b[0;36mget_importance_trace\u001b[0;34m(graph_type, max_plate_nesting, model, guide, *args, **kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0magainst\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \"\"\"\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mguide_trace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoutine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mguide\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgraph_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     model_trace = poutine.trace(poutine.replay(model, trace=guide_trace),\n\u001b[1;32m     44\u001b[0m                                 graph_type=graph_type).get_trace(*args, **kwargs)\n",
      "\u001b[0;32m~/.virtualenvs/pp/lib/python3.6/site-packages/pyro/poutine/trace_messenger.py\u001b[0m in \u001b[0;36mget_trace\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mCalls\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mpoutine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0mits\u001b[0m \u001b[0mtrace\u001b[0m \u001b[0minstead\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \"\"\"\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsngr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pp/lib/python3.6/site-packages/pyro/poutine/trace_messenger.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m                                       args=args, kwargs=kwargs)\n\u001b[1;32m    146\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pp/lib/python3.6/site-packages/pyro/contrib/autoguide/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;31m# if we've never run the model before, do so now so we can inspect the model structure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprototype_trace\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_prototype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mlatent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_latent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pp/lib/python3.6/site-packages/pyro/contrib/autoguide/__init__.py\u001b[0m in \u001b[0;36m_setup_prototype\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setup_prototype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAutoContinuous\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setup_prototype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unconstrained_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond_indep_stacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pp/lib/python3.6/site-packages/pyro/contrib/autoguide/__init__.py\u001b[0m in \u001b[0;36m_setup_prototype\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setup_prototype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;31m# run the model so we can inspect its structure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprototype_trace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoutine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoutine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprototype_trace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprune_subsample_sites\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprototype_trace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pp/lib/python3.6/site-packages/pyro/poutine/messenger.py\u001b[0m in \u001b[0;36m_wraps\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_wraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0m_wraps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsngr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wraps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pp/lib/python3.6/site-packages/pyro/poutine/trace_messenger.py\u001b[0m in \u001b[0;36mget_trace\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mCalls\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mpoutine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0mits\u001b[0m \u001b[0mtrace\u001b[0m \u001b[0minstead\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \"\"\"\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmsngr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pp/lib/python3.6/site-packages/pyro/poutine/trace_messenger.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    145\u001b[0m                                       args=args, kwargs=kwargs)\n\u001b[1;32m    146\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m                 \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-4656d09c749e>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(x_data, y_data)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sigma\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# lift module parameters to random variables sampled from the priors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mlifted_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyro\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"module\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregression_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpriors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;31m# sample a nn (which also samples w and b)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mlifted_reg_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlifted_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'regression_model' is not defined"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    pyro.clear_param_store()\n",
    "    for j in range(num_iterations):\n",
    "        # calculate the loss and take a gradient step\n",
    "        loss = svi.step(x_data, y_data)\n",
    "        if j % 100 == 0:\n",
    "            print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss / len(data)))\n",
    "\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, value in pyro.get_param_store().items():\n",
    "    print(name, pyro.param(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_marginal = lambda traces, sites:EmpiricalMarginal(traces, sites)._get_samples_and_weights()[0].detach().cpu().numpy()\n",
    "\n",
    "def summary(traces, sites):\n",
    "    marginal = get_marginal(traces, sites)\n",
    "    site_stats = {}\n",
    "    for i in range(marginal.shape[1]):\n",
    "        site_name = sites[i]\n",
    "        marginal_site = pd.DataFrame(marginal[:, i]).transpose()\n",
    "        describe = partial(pd.Series.describe, percentiles=[.05, 0.25, 0.5, 0.75, 0.95])\n",
    "        site_stats[site_name] = marginal_site.apply(describe, axis=1) \\\n",
    "            [[\"mean\", \"std\", \"5%\", \"25%\", \"50%\", \"75%\", \"95%\"]]\n",
    "    return site_stats\n",
    "\n",
    "def wrapped_model(x_data, y_data):\n",
    "    pyro.sample(\"prediction\", Delta(model(x_data, y_data)))\n",
    "\n",
    "posterior = svi.run(x_data, y_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# posterior predictive distribution we can get samples from\n",
    "trace_pred = TracePredictive(wrapped_model,\n",
    "                             posterior,\n",
    "                             num_samples=1000)\n",
    "post_pred = trace_pred.run(x_data, None)\n",
    "post_summary = summary(post_pred, sites= ['prediction', 'obs'])\n",
    "mu = post_summary[\"prediction\"]\n",
    "y = post_summary[\"obs\"]\n",
    "predictions = pd.DataFrame({\n",
    "    \"cont_africa\": x_data[:, 0],\n",
    "    \"rugged\": x_data[:, 1],\n",
    "    \"mu_mean\": mu[\"mean\"],\n",
    "    \"mu_perc_5\": mu[\"5%\"],\n",
    "    \"mu_perc_95\": mu[\"95%\"],\n",
    "    \"y_mean\": y[\"mean\"],\n",
    "    \"y_perc_5\": y[\"5%\"],\n",
    "    \"y_perc_95\": y[\"95%\"],\n",
    "    \"true_gdp\": y_data,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6), sharey=True)\n",
    "african_nations = predictions[predictions[\"cont_africa\"] == 1]\n",
    "non_african_nations = predictions[predictions[\"cont_africa\"] == 0]\n",
    "african_nations = african_nations.sort_values(by=[\"rugged\"])\n",
    "non_african_nations = non_african_nations.sort_values(by=[\"rugged\"])\n",
    "fig.suptitle(\"Regression line 90% CI\", fontsize=16)\n",
    "ax[0].plot(non_african_nations[\"rugged\"],\n",
    "           non_african_nations[\"mu_mean\"])\n",
    "ax[0].fill_between(non_african_nations[\"rugged\"],\n",
    "                   non_african_nations[\"mu_perc_5\"],\n",
    "                   non_african_nations[\"mu_perc_95\"],\n",
    "                   alpha=0.5)\n",
    "ax[0].plot(non_african_nations[\"rugged\"],\n",
    "           non_african_nations[\"true_gdp\"],\n",
    "           \"o\")\n",
    "ax[0].set(xlabel=\"Terrain Ruggedness Index\",\n",
    "          ylabel=\"log GDP (2000)\",\n",
    "          title=\"Non African Nations\")\n",
    "idx = np.argsort(african_nations[\"rugged\"])\n",
    "ax[1].plot(african_nations[\"rugged\"],\n",
    "           african_nations[\"mu_mean\"])\n",
    "ax[1].fill_between(african_nations[\"rugged\"],\n",
    "                   african_nations[\"mu_perc_5\"],\n",
    "                   african_nations[\"mu_perc_95\"],\n",
    "                   alpha=0.5)\n",
    "ax[1].plot(african_nations[\"rugged\"],\n",
    "           african_nations[\"true_gdp\"],\n",
    "           \"o\")\n",
    "ax[1].set(xlabel=\"Terrain Ruggedness Index\",\n",
    "          ylabel=\"log GDP (2000)\",\n",
    "          title=\"African Nations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(12, 6), sharey=True)\n",
    "fig.suptitle(\"Posterior predictive distribution with 90% CI\", fontsize=16)\n",
    "ax[0].plot(non_african_nations[\"rugged\"],\n",
    "           non_african_nations[\"y_mean\"])\n",
    "ax[0].fill_between(non_african_nations[\"rugged\"],\n",
    "                   non_african_nations[\"y_perc_5\"],\n",
    "                   non_african_nations[\"y_perc_95\"],\n",
    "                   alpha=0.5)\n",
    "ax[0].plot(non_african_nations[\"rugged\"],\n",
    "           non_african_nations[\"true_gdp\"],\n",
    "           \"o\")\n",
    "ax[0].set(xlabel=\"Terrain Ruggedness Index\",\n",
    "          ylabel=\"log GDP (2000)\",\n",
    "          title=\"Non African Nations\")\n",
    "idx = np.argsort(african_nations[\"rugged\"])\n",
    "\n",
    "ax[1].plot(african_nations[\"rugged\"],\n",
    "           african_nations[\"y_mean\"])\n",
    "ax[1].fill_between(african_nations[\"rugged\"],\n",
    "                   african_nations[\"y_perc_5\"],\n",
    "                   african_nations[\"y_perc_95\"],\n",
    "                   alpha=0.5)\n",
    "ax[1].plot(african_nations[\"rugged\"],\n",
    "           african_nations[\"true_gdp\"],\n",
    "           \"o\")\n",
    "ax[1].set(xlabel=\"Terrain Ruggedness Index\",\n",
    "          ylabel=\"log GDP (2000)\",\n",
    "          title=\"African Nations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to prepend `module$$$` to all parameters of nn.Modules since\n",
    "# that is how they are stored in the ParamStore\n",
    "weight = get_marginal(posterior, ['module$$$linear.weight']).squeeze(1).squeeze(1)\n",
    "factor = get_marginal(posterior, ['module$$$factor'])\n",
    "gamma_within_africa = weight[:, 1] + factor.squeeze(1)\n",
    "gamma_outside_africa = weight[:, 1]\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "sns.distplot(gamma_within_africa, kde_kws={\"label\": \"African nations\"},)\n",
    "sns.distplot(gamma_outside_africa, kde_kws={\"label\": \"Non-African nations\"})\n",
    "fig.suptitle(\"Density of Slope : log(GDP) vs. Terrain Ruggedness\", fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def argmax(vec):\n",
    "    # return the argmax as a python int\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    return idx.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
    "def log_sum_exp(vec):\n",
    "    max_score = vec[0, argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + \\\n",
    "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from PyTorch tutorials: https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        # tagset_size includes <START> and <STOP>\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "\n",
    "        # embedding matrix\n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
    "                            num_layers=1, bidirectional=True)\n",
    "\n",
    "        # project the output of the LSTM at each state into tag space. Input to CRF model.\n",
    "        # fully connected layers, emission score per token\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
    "\n",
    "        # Matrix of transition parameters. Entry i,j is the score of\n",
    "        # transitioning *to* i *from* j.\n",
    "        self.transitions = nn.Parameter(\n",
    "            torch.randn(self.tagset_size, self.tagset_size))\n",
    "\n",
    "        # These two statements enforce the constraint that we never transfer\n",
    "        # to the start tag and we never transfer from the stop tag\n",
    "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
    "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
    "\n",
    "        # this is NOT a right stochastic matrix whose row sums to 1, simply scores.\n",
    "\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.randn(2, 1, self.hidden_dim // 2),\n",
    "                torch.randn(2, 1, self.hidden_dim // 2))\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        # Do the forward algorithm to compute the partition function\n",
    "        init_alphas = torch.full((1, self.tagset_size), -10000.)\n",
    "        # START_TAG has all of the score.\n",
    "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "\n",
    "        # Wrap in a variable so that we will get automatic backprop\n",
    "        forward_var = init_alphas\n",
    "\n",
    "        # Iterate through the sentence\n",
    "        for feat in feats:\n",
    "            alphas_t = []  # The forward tensors at this timestep\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # broadcast the emission score: it is the same regardless of\n",
    "                # the previous tag\n",
    "                emit_score = feat[next_tag].view(\n",
    "                    1, -1).expand(1, self.tagset_size)\n",
    "                # the ith entry of trans_score is the score of transitioning to\n",
    "                # next_tag from i\n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                # The ith entry of next_tag_var is the value for the\n",
    "                # edge (i -> next_tag) before we do log-sum-exp\n",
    "                next_tag_var = forward_var + trans_score + emit_score\n",
    "                # The forward variable for this tag is log-sum-exp of all the\n",
    "                # scores.\n",
    "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha\n",
    "\n",
    "    def _penalty(self, lda=1e-2, norm=\"l1\"):\n",
    "        w = self.transitions.detach()\n",
    "        w[tag_to_ix[START_TAG], :] = 0\n",
    "        w[:, tag_to_ix[STOP_TAG]] = 0\n",
    "        \n",
    "        if norm == \"l1\":\n",
    "            return w.abs().sum() * lda\n",
    "        elif norm == \"l2\":\n",
    "            return torch.pow(w, 2).sum().sqrt() * lda\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def _get_lstm_features(self, sentence):\n",
    "        self.hidden = self.init_hidden()\n",
    "        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n",
    "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
    "        # output [len(sentences), hidden_dim]\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "        # lstm_feats [len(sentence), len(tag_size)] aka emission score\n",
    "        return lstm_feats\n",
    "\n",
    "    def _score_sentence(self, feats, tags):\n",
    "        # feats in shape [len(sentences), len(tag_size)]\n",
    "        # Gives the score of a provided tag sequence\n",
    "        score = torch.zeros(1)\n",
    "        # insert corresponding index of START_TAG\n",
    "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n",
    "        for i, feat in enumerate(feats):\n",
    "            # feat in [hidden_size], emission score from LSTM\n",
    "            score += self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "        score += self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
    "        # total of emission score and transition score, given the sequence and state transitions.\n",
    "        return score\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "\n",
    "        # Initialize the viterbi variables in log space\n",
    "        init_vvars = torch.full((1, self.tagset_size), -10000.)\n",
    "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "\n",
    "        # forward_var at step i holds the viterbi variables for step i-1\n",
    "        forward_var = init_vvars\n",
    "        for feat in feats:\n",
    "            bptrs_t = []  # holds the backpointers for this step\n",
    "            viterbivars_t = []  # holds the viterbi variables for this step\n",
    "\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
    "                # previous step, plus the score of transitioning\n",
    "                # from tag i to next_tag.\n",
    "                # We don't include the emission scores here because the max\n",
    "                # does not depend on them (we add them in below)\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                best_tag_id = argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "            # Now add in the emission scores, and assign forward_var to the set\n",
    "            # of viterbi variables we just computed\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        # Transition to STOP_TAG\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        best_tag_id = argmax(terminal_var)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "        # Follow the back pointers to decode the best ptranath.\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        # Pop off the start tag (we dont want to return that to the caller)\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "\n",
    "    def neg_log_likelihood(self, sentence, tags):\n",
    "        # Lafferty et al. 2001\n",
    "        # sentences are index-of-vocabulary, so are tags\n",
    "        feats = self._get_lstm_features(sentence)\n",
    "        # feats [len(sentence), len(tag_size)]\n",
    "        # probability of the sequence given parameters (of BiLSTM and CRF)\n",
    "        forward_score = self._forward_alg(feats)\n",
    "        # if states (i.e. tags) are given, probability of sequence (i.e. pure emission scores)\n",
    "        gold_score = self._score_sentence(feats, tags)\n",
    "        return forward_score - gold_score + self._penalty()\n",
    "\n",
    "    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
    "        # Get the emission scores from the BiLSTM\n",
    "        lstm_feats = self._get_lstm_features(sentence)\n",
    "\n",
    "        # Find the best path, given the features.\n",
    "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
    "        return score, tag_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jsonl_to_bilou(iterable: List[Dict]) -> Tuple:\n",
    "    for i in iterable:\n",
    "        tokens = i[\"paragraphs\"][0][\"sentences\"][0][\"tokens\"]\n",
    "        words = [token[\"orth\"] for token in tokens]\n",
    "        entities = [token[\"ner\"] for token in tokens]\n",
    "        yield tuple([words, entities])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "EMBEDDING_DIM = 10\n",
    "HIDDEN_DIM = 8\n",
    "\n",
    "# Make up some training data\n",
    "# training_data = [(\n",
    "#     \"the wall street journal reported today that apple corporation made money\".split(),\n",
    "#     \"B I I I O O O B I O O\".split()\n",
    "# ), (\n",
    "#     \"georgia tech is a university in georgia\".split(),\n",
    "#     \"B I O O O O B\".split()\n",
    "# )]\n",
    "\n",
    "with open(Path(\"./data/onto.jsonl/onto.train.json\"), \"r\") as f:\n",
    "    raw_train_json = json.load(f)\n",
    "    training_data = list(jsonl_to_bilou(raw_train_json))[:50]\n",
    "\n",
    "# with open(Path(\"./data/onto.jsonl/onto.development.json\"), \"r\") as f:\n",
    "#     raw_dev_json = json.load(f)\n",
    "#     validation_data = list(jsonl_to_bilou(raw_dev_json))\n",
    "\n",
    "word_to_ix = {}\n",
    "tag_to_ix = {}\n",
    "for sentence, tags in training_data:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "    for tag in tags:\n",
    "        if tag not in tag_to_ix:\n",
    "            tag_to_ix[tag] = len(tag_to_ix)\n",
    "\n",
    "# for sentence, tags in validation_data:\n",
    "#     for word in sentence:\n",
    "#         if word not in word_to_ix:\n",
    "#             word_to_ix[word] = len(word_to_ix)\n",
    "#     for tag in tags:\n",
    "#         if tag not in tag_to_ix:\n",
    "#             tag_to_ix[tag] = len(tag_to_ix)\n",
    "\n",
    "tag_to_ix[START_TAG] = len(tag_to_ix)\n",
    "tag_to_ix[STOP_TAG] = len(tag_to_ix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check predictions before training\n",
    "with torch.no_grad():\n",
    "    precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    precheck_tags = torch.tensor([tag_to_ix[t] for t in training_data[0][1]], dtype=torch.long)\n",
    "    print(model(precheck_sent))\n",
    "    print(prepare_sequence(training_data[0][1], tag_to_ix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(30):\n",
    "    for k, (sentence, tags) in enumerate(training_data):\n",
    "        model.zero_grad()\n",
    "\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        loss = model.neg_log_likelihood(sentence_in, targets)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tag_to_ix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check predictions after training\n",
    "with torch.no_grad():\n",
    "    for i in range(len(training_data)):\n",
    "        precheck_sent = prepare_sequence(training_data[i][0], word_to_ix)\n",
    "        print(model(precheck_sent))\n",
    "        print(prepare_sequence(training_data[i][1], tag_to_ix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty = model.transitions.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.embeddings import WordEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embedding = WordEmbeddings(\"glove\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"we are travelling to Washington .\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Sentence(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s[-2].get_embedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding.embed([s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s[-2].get_embedding().size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat([token.get_embedding().unsqueeze(0) for token in s], 0).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"training.log\", \"r\") as f:\n",
    "    no_penalty_0 = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"training_1.log\", \"r\") as f:\n",
    "    no_penalty_1 = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"training_l1.log\", \"r\") as f:\n",
    "    l1_penalty_0 = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"training_l1_1.log\", \"r\") as f:\n",
    "    l1_penalty_1 = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"training_2.log\", \"r\") as f:\n",
    "    no_penalty_2 = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"training_l1_2.log\", \"r\") as f:\n",
    "    l1_penalty_2 = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"training_3.log\", \"r\") as f:\n",
    "    no_penalty_3 = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"training_l1_3.log\", \"r\") as f:\n",
    "    l1_penalty_3 = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"training_4.log\", \"r\") as f:\n",
    "    no_penalty_4 = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"training_l1_4.log\", \"r\") as f:\n",
    "    l1_penalty_4 = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"training_5.log\", \"r\") as f:\n",
    "    no_penalty_5 = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"training_l2_5.log\", \"r\") as f:\n",
    "    l2_penalty_5 = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"training_6.log\", \"r\") as f:\n",
    "    no_penalty_6 = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"training_l2_6.log\", \"r\") as f:\n",
    "    l2_penalty_6 = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"training_7.log\", \"r\") as f:\n",
    "    no_penalty_7 = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"training_l2_7.log\", \"r\") as f:\n",
    "    l2_penalty_7 = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"training_8.log\", \"r\") as f:\n",
    "    no_penalty_8 = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"training_l2_w_8.log\", \"r\") as f:\n",
    "    l2_penalty_w_8 = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"training_9.log\", \"r\") as f:\n",
    "    no_penalty_9 = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"training_l2_w_9.log\", \"r\") as f:\n",
    "    l2_penalty_w_9 = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"training_10.log\", \"r\") as f:\n",
    "    no_penalty_10 = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"training_l2_w_10.log\", \"r\") as f:\n",
    "    l2_penalty_w_10 = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_scores(lines):\n",
    "    _breaker = \"MACRO_AVG: acc\"\n",
    "    CAP = False\n",
    "    sep2 = \" - \"\n",
    "    sep3 = \": \"\n",
    "    summary = {}\n",
    "    for l in lines:\n",
    "        if _breaker in l:\n",
    "            CAP = True\n",
    "            continue\n",
    "        if CAP:\n",
    "            try:\n",
    "                sep1 = l.index(\"tp:\")\n",
    "            except ValueError:\n",
    "                CAP = False\n",
    "                continue\n",
    "            l = l.strip()\n",
    "            entity, rest = l[:sep1].strip(), l[sep1:]\n",
    "            summary[entity] = dict([i.split(sep3) for i in rest.split(sep2)])\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_objects = [i for i in locals() if \"penalty\" in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_penalty = [i for i in p_objects if i.startswith(\"l\") and \"_penalty\" in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "without_penalty = [i for i in p_objects if \"no_penalty\" in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_results(penalty, no_penalty):\n",
    "    p_lst = []\n",
    "    no_p_lst = []\n",
    "\n",
    "    for k, (p, no_p) in enumerate(zip(penalty, no_penalty)):\n",
    "        \n",
    "        p_df = _get_scores(eval(p))\n",
    "        p_df = pd.DataFrame(p_df)\n",
    "        p_df[\"EXPERIMENT\"] = k\n",
    "        p_df[\"PENALTY\"] = True\n",
    "        p_lst.append(p_df)\n",
    "        \n",
    "        df = _get_scores(eval(no_p))\n",
    "        df = pd.DataFrame(df)\n",
    "        df[\"EXPERIMENT\"] = k\n",
    "        df[\"PENALTY\"] = False\n",
    "        no_p_lst.append(df)\n",
    "\n",
    "    rv = pd.concat(p_lst + no_p_lst)\n",
    "    rv = rv.drop(rv.index.difference([\"f1-score\"]), axis=0).reset_index(drop=True)\n",
    "    return rv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['l1_penalty_0',\n",
       " 'l1_penalty_1',\n",
       " 'l1_penalty_2',\n",
       " 'l1_penalty_3',\n",
       " 'l1_penalty_4',\n",
       " 'l2_penalty_5',\n",
       " 'l2_penalty_6',\n",
       " 'l2_penalty_7',\n",
       " 'l2_penalty_w_8',\n",
       " 'l2_penalty_w_9',\n",
       " 'l2_penalty_w_10']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['no_penalty_0',\n",
       " 'no_penalty_1',\n",
       " 'no_penalty_2',\n",
       " 'no_penalty_3',\n",
       " 'no_penalty_4',\n",
       " 'no_penalty_5',\n",
       " 'no_penalty_6',\n",
       " 'no_penalty_7',\n",
       " 'no_penalty_8',\n",
       " 'no_penalty_9',\n",
       " 'no_penalty_10']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "without_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "assm_df = assemble_results(with_penalty, without_penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CARDINAL</th>\n",
       "      <th>DATE</th>\n",
       "      <th>EVENT</th>\n",
       "      <th>FAC</th>\n",
       "      <th>GPE</th>\n",
       "      <th>LANGUAGE</th>\n",
       "      <th>LAW</th>\n",
       "      <th>LOC</th>\n",
       "      <th>MONEY</th>\n",
       "      <th>NORP</th>\n",
       "      <th>ORDINAL</th>\n",
       "      <th>ORG</th>\n",
       "      <th>PERCENT</th>\n",
       "      <th>PERSON</th>\n",
       "      <th>PRODUCT</th>\n",
       "      <th>QUANTITY</th>\n",
       "      <th>TIME</th>\n",
       "      <th>WORK_OF_ART</th>\n",
       "      <th>EXPERIMENT</th>\n",
       "      <th>PENALTY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.5827</td>\n",
       "      <td>0.6599</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1900</td>\n",
       "      <td>0.7805</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0400</td>\n",
       "      <td>0.3387</td>\n",
       "      <td>0.7088</td>\n",
       "      <td>0.7436</td>\n",
       "      <td>0.5897</td>\n",
       "      <td>0.6571</td>\n",
       "      <td>0.6445</td>\n",
       "      <td>0.7615</td>\n",
       "      <td>0.1986</td>\n",
       "      <td>0.0693</td>\n",
       "      <td>0.2037</td>\n",
       "      <td>0.1606</td>\n",
       "      <td>6</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.5813</td>\n",
       "      <td>0.6795</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3231</td>\n",
       "      <td>0.7719</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1208</td>\n",
       "      <td>0.6104</td>\n",
       "      <td>0.7239</td>\n",
       "      <td>0.5459</td>\n",
       "      <td>0.6324</td>\n",
       "      <td>0.6637</td>\n",
       "      <td>0.7736</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2017</td>\n",
       "      <td>0.2787</td>\n",
       "      <td>0.0960</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.5282</td>\n",
       "      <td>0.6220</td>\n",
       "      <td>0.2087</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.7851</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2222</td>\n",
       "      <td>0.4158</td>\n",
       "      <td>0.6708</td>\n",
       "      <td>0.7842</td>\n",
       "      <td>0.5915</td>\n",
       "      <td>0.6346</td>\n",
       "      <td>0.6254</td>\n",
       "      <td>0.7973</td>\n",
       "      <td>0.2131</td>\n",
       "      <td>0.2470</td>\n",
       "      <td>0.3100</td>\n",
       "      <td>0.2803</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.6201</td>\n",
       "      <td>0.6892</td>\n",
       "      <td>0.2295</td>\n",
       "      <td>0.2832</td>\n",
       "      <td>0.7800</td>\n",
       "      <td>0.3226</td>\n",
       "      <td>0.0384</td>\n",
       "      <td>0.3561</td>\n",
       "      <td>0.7325</td>\n",
       "      <td>0.7690</td>\n",
       "      <td>0.5579</td>\n",
       "      <td>0.6158</td>\n",
       "      <td>0.7059</td>\n",
       "      <td>0.7565</td>\n",
       "      <td>0.1449</td>\n",
       "      <td>0.3832</td>\n",
       "      <td>0.3029</td>\n",
       "      <td>0.1530</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.5860</td>\n",
       "      <td>0.6693</td>\n",
       "      <td>0.1222</td>\n",
       "      <td>0.1932</td>\n",
       "      <td>0.7898</td>\n",
       "      <td>0.3572</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2869</td>\n",
       "      <td>0.4765</td>\n",
       "      <td>0.7451</td>\n",
       "      <td>0.5885</td>\n",
       "      <td>0.6374</td>\n",
       "      <td>0.7205</td>\n",
       "      <td>0.7729</td>\n",
       "      <td>0.1887</td>\n",
       "      <td>0.4674</td>\n",
       "      <td>0.3158</td>\n",
       "      <td>0.2384</td>\n",
       "      <td>10</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.6010</td>\n",
       "      <td>0.6693</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1947</td>\n",
       "      <td>0.7843</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0392</td>\n",
       "      <td>0.3632</td>\n",
       "      <td>0.6936</td>\n",
       "      <td>0.7519</td>\n",
       "      <td>0.5721</td>\n",
       "      <td>0.6438</td>\n",
       "      <td>0.6531</td>\n",
       "      <td>0.7890</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0.2136</td>\n",
       "      <td>0.1918</td>\n",
       "      <td>6</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.5585</td>\n",
       "      <td>0.6625</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2782</td>\n",
       "      <td>0.7458</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0772</td>\n",
       "      <td>0.6213</td>\n",
       "      <td>0.7134</td>\n",
       "      <td>0.4767</td>\n",
       "      <td>0.5928</td>\n",
       "      <td>0.6543</td>\n",
       "      <td>0.7560</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1389</td>\n",
       "      <td>0.2135</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.5014</td>\n",
       "      <td>0.6219</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0382</td>\n",
       "      <td>0.7608</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0471</td>\n",
       "      <td>0.3225</td>\n",
       "      <td>0.6577</td>\n",
       "      <td>0.7796</td>\n",
       "      <td>0.5820</td>\n",
       "      <td>0.6319</td>\n",
       "      <td>0.6271</td>\n",
       "      <td>0.7834</td>\n",
       "      <td>0.0521</td>\n",
       "      <td>0.1407</td>\n",
       "      <td>0.0655</td>\n",
       "      <td>0.2074</td>\n",
       "      <td>8</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.6310</td>\n",
       "      <td>0.6906</td>\n",
       "      <td>0.2147</td>\n",
       "      <td>0.1762</td>\n",
       "      <td>0.7776</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2571</td>\n",
       "      <td>0.7227</td>\n",
       "      <td>0.7828</td>\n",
       "      <td>0.5048</td>\n",
       "      <td>0.6393</td>\n",
       "      <td>0.6998</td>\n",
       "      <td>0.7627</td>\n",
       "      <td>0.0210</td>\n",
       "      <td>0.2960</td>\n",
       "      <td>0.2320</td>\n",
       "      <td>0.1185</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.5610</td>\n",
       "      <td>0.6756</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.0459</td>\n",
       "      <td>0.7726</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1325</td>\n",
       "      <td>0.5019</td>\n",
       "      <td>0.7508</td>\n",
       "      <td>0.5075</td>\n",
       "      <td>0.6288</td>\n",
       "      <td>0.7011</td>\n",
       "      <td>0.7894</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1730</td>\n",
       "      <td>0.1484</td>\n",
       "      <td>0.1308</td>\n",
       "      <td>10</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CARDINAL    DATE   EVENT     FAC     GPE LANGUAGE     LAW     LOC   MONEY  \\\n",
       "6    0.5827  0.6599  0.0000  0.1900  0.7805   0.0000  0.0400  0.3387  0.7088   \n",
       "7    0.5813  0.6795  0.0000  0.3231  0.7719   0.0000  0.0000  0.1208  0.6104   \n",
       "8    0.5282  0.6220  0.2087  0.2500  0.7851   0.0000  0.2222  0.4158  0.6708   \n",
       "9    0.6201  0.6892  0.2295  0.2832  0.7800   0.3226  0.0384  0.3561  0.7325   \n",
       "10   0.5860  0.6693  0.1222  0.1932  0.7898   0.3572  0.0000  0.2869  0.4765   \n",
       "17   0.6010  0.6693  0.0000  0.1947  0.7843   0.0000  0.0392  0.3632  0.6936   \n",
       "18   0.5585  0.6625  0.0000  0.2782  0.7458   0.0000  0.0000  0.0772  0.6213   \n",
       "19   0.5014  0.6219  0.0223  0.0382  0.7608   0.0000  0.0471  0.3225  0.6577   \n",
       "20   0.6310  0.6906  0.2147  0.1762  0.7776   0.0000  0.0000  0.2571  0.7227   \n",
       "21   0.5610  0.6756  0.0649  0.0459  0.7726   0.0000  0.0000  0.1325  0.5019   \n",
       "\n",
       "      NORP ORDINAL     ORG PERCENT  PERSON PRODUCT QUANTITY    TIME  \\\n",
       "6   0.7436  0.5897  0.6571  0.6445  0.7615  0.1986   0.0693  0.2037   \n",
       "7   0.7239  0.5459  0.6324  0.6637  0.7736  0.0000   0.2017  0.2787   \n",
       "8   0.7842  0.5915  0.6346  0.6254  0.7973  0.2131   0.2470  0.3100   \n",
       "9   0.7690  0.5579  0.6158  0.7059  0.7565  0.1449   0.3832  0.3029   \n",
       "10  0.7451  0.5885  0.6374  0.7205  0.7729  0.1887   0.4674  0.3158   \n",
       "17  0.7519  0.5721  0.6438  0.6531  0.7890  0.1875   0.0117  0.2136   \n",
       "18  0.7134  0.4767  0.5928  0.6543  0.7560  0.0000   0.1389  0.2135   \n",
       "19  0.7796  0.5820  0.6319  0.6271  0.7834  0.0521   0.1407  0.0655   \n",
       "20  0.7828  0.5048  0.6393  0.6998  0.7627  0.0210   0.2960  0.2320   \n",
       "21  0.7508  0.5075  0.6288  0.7011  0.7894  0.0000   0.1730  0.1484   \n",
       "\n",
       "   WORK_OF_ART  EXPERIMENT  PENALTY  \n",
       "6       0.1606           6     True  \n",
       "7       0.0960           7     True  \n",
       "8       0.2803           8     True  \n",
       "9       0.1530           9     True  \n",
       "10      0.2384          10     True  \n",
       "17      0.1918           6    False  \n",
       "18      0.0000           7    False  \n",
       "19      0.2074           8    False  \n",
       "20      0.1185           9    False  \n",
       "21      0.1308          10    False  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assm_df.loc[assm_df[\"EXPERIMENT\"] >= 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"TRAIN\": {\n",
      "        \"dataset\": \"TRAIN\",\n",
      "        \"total_number_of_documents\": 2316,\n",
      "        \"number_of_documents_per_class\": {},\n",
      "        \"number_of_tokens_per_tag\": {\n",
      "            \"O\": 39039,\n",
      "            \"B-ORG\": 514,\n",
      "            \"B-PERSON\": 466,\n",
      "            \"I-PERSON\": 331,\n",
      "            \"I-ORG\": 643,\n",
      "            \"B-DATE\": 366,\n",
      "            \"B-PERCENT\": 71,\n",
      "            \"I-PERCENT\": 116,\n",
      "            \"B-CARDINAL\": 209,\n",
      "            \"I-CARDINAL\": 72,\n",
      "            \"I-DATE\": 399,\n",
      "            \"B-TIME\": 35,\n",
      "            \"B-GPE\": 470,\n",
      "            \"I-GPE\": 133,\n",
      "            \"B-NORP\": 185,\n",
      "            \"B-LOC\": 46,\n",
      "            \"I-LOC\": 59,\n",
      "            \"B-MONEY\": 87,\n",
      "            \"I-MONEY\": 132,\n",
      "            \"B-PRODUCT\": 24,\n",
      "            \"B-LANGUAGE\": 7,\n",
      "            \"B-QUANTITY\": 21,\n",
      "            \"I-TIME\": 20,\n",
      "            \"B-EVENT\": 23,\n",
      "            \"I-EVENT\": 54,\n",
      "            \"B-FAC\": 20,\n",
      "            \"I-FAC\": 31,\n",
      "            \"I-NORP\": 15,\n",
      "            \"B-ORDINAL\": 38,\n",
      "            \"I-PRODUCT\": 13,\n",
      "            \"B-WORK_OF_ART\": 27,\n",
      "            \"I-WORK_OF_ART\": 56,\n",
      "            \"I-QUANTITY\": 29,\n",
      "            \"B-LAW\": 8,\n",
      "            \"I-LAW\": 14\n",
      "        },\n",
      "        \"number_of_tokens\": {\n",
      "            \"total\": 43773,\n",
      "            \"min\": 1,\n",
      "            \"max\": 145,\n",
      "            \"avg\": 18.900259067357513\n",
      "        }\n",
      "    },\n",
      "    \"TEST\": {\n",
      "        \"dataset\": \"TEST\",\n",
      "        \"total_number_of_documents\": 12217,\n",
      "        \"number_of_documents_per_class\": {},\n",
      "        \"number_of_tokens_per_tag\": {\n",
      "            \"O\": 206792,\n",
      "            \"B-NORP\": 990,\n",
      "            \"B-ORG\": 2002,\n",
      "            \"I-ORG\": 2703,\n",
      "            \"B-PERSON\": 2134,\n",
      "            \"I-PERSON\": 1512,\n",
      "            \"B-DATE\": 1787,\n",
      "            \"B-GPE\": 2546,\n",
      "            \"I-DATE\": 2197,\n",
      "            \"B-FAC\": 149,\n",
      "            \"I-FAC\": 243,\n",
      "            \"I-GPE\": 717,\n",
      "            \"B-CARDINAL\": 1005,\n",
      "            \"B-TIME\": 225,\n",
      "            \"I-TIME\": 271,\n",
      "            \"B-ORDINAL\": 207,\n",
      "            \"B-EVENT\": 85,\n",
      "            \"I-EVENT\": 165,\n",
      "            \"I-CARDINAL\": 370,\n",
      "            \"B-QUANTITY\": 153,\n",
      "            \"I-QUANTITY\": 262,\n",
      "            \"B-PERCENT\": 408,\n",
      "            \"I-PERCENT\": 619,\n",
      "            \"B-LOC\": 215,\n",
      "            \"I-LOC\": 202,\n",
      "            \"B-WORK_OF_ART\": 169,\n",
      "            \"I-WORK_OF_ART\": 347,\n",
      "            \"B-MONEY\": 353,\n",
      "            \"I-MONEY\": 772,\n",
      "            \"B-LAW\": 44,\n",
      "            \"I-LAW\": 117,\n",
      "            \"I-NORP\": 162,\n",
      "            \"I-ORDINAL\": 6,\n",
      "            \"B-PRODUCT\": 90,\n",
      "            \"I-PRODUCT\": 70,\n",
      "            \"B-LANGUAGE\": 22\n",
      "        },\n",
      "        \"number_of_tokens\": {\n",
      "            \"total\": 230111,\n",
      "            \"min\": 1,\n",
      "            \"max\": 151,\n",
      "            \"avg\": 18.835311451256445\n",
      "        }\n",
      "    },\n",
      "    \"DEV\": {\n",
      "        \"dataset\": \"DEV\",\n",
      "        \"total_number_of_documents\": 15680,\n",
      "        \"number_of_documents_per_class\": {},\n",
      "        \"number_of_tokens_per_tag\": {\n",
      "            \"O\": 267167,\n",
      "            \"B-DATE\": 3208,\n",
      "            \"I-DATE\": 3484,\n",
      "            \"B-GPE\": 3649,\n",
      "            \"I-GPE\": 906,\n",
      "            \"B-ORDINAL\": 335,\n",
      "            \"B-ORG\": 3798,\n",
      "            \"B-FAC\": 133,\n",
      "            \"I-FAC\": 228,\n",
      "            \"I-ORG\": 5166,\n",
      "            \"B-QUANTITY\": 190,\n",
      "            \"I-QUANTITY\": 332,\n",
      "            \"B-LOC\": 316,\n",
      "            \"I-LOC\": 280,\n",
      "            \"B-CARDINAL\": 1719,\n",
      "            \"B-NORP\": 1277,\n",
      "            \"B-LAW\": 65,\n",
      "            \"I-LAW\": 174,\n",
      "            \"I-CARDINAL\": 562,\n",
      "            \"B-MONEY\": 842,\n",
      "            \"I-MONEY\": 1452,\n",
      "            \"B-TIME\": 361,\n",
      "            \"B-EVENT\": 179,\n",
      "            \"I-EVENT\": 325,\n",
      "            \"B-WORK_OF_ART\": 202,\n",
      "            \"I-WORK_OF_ART\": 437,\n",
      "            \"B-PERSON\": 3163,\n",
      "            \"I-PERSON\": 2191,\n",
      "            \"I-TIME\": 398,\n",
      "            \"B-PERCENT\": 656,\n",
      "            \"I-PERCENT\": 910,\n",
      "            \"I-NORP\": 68,\n",
      "            \"B-PRODUCT\": 214,\n",
      "            \"I-PRODUCT\": 257,\n",
      "            \"B-LANGUAGE\": 35,\n",
      "            \"I-ORDINAL\": 4,\n",
      "            \"I-LANGUAGE\": 1\n",
      "        },\n",
      "        \"number_of_tokens\": {\n",
      "            \"total\": 304684,\n",
      "            \"min\": 1,\n",
      "            \"max\": 275,\n",
      "            \"avg\": 19.431377551020407\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open(\"corpus_l2_w_10.log\") as f:\n",
    "    with open(\"corpus_10.log\") as w:\n",
    "        z = f.read()\n",
    "        assert z == w.read()\n",
    "        print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
