{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pyro, Bayesian analysis, and attempt on BCRF (Qi et al 2005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "from pyro.distributions import Normal, Uniform\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, Trace_ELBO, EmpiricalMarginal, TraceEnum_ELBO, JitTraceEnum_ELBO\n",
    "from pyro.infer.mcmc import MCMC, NUTS\n",
    "from pyro.optim import Adam\n",
    "from pyro.util import ignore_jit_warnings\n",
    "from pyro.contrib.autoguide import AutoDelta\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA_URL = \"https://d2fefpcigoriu7.cloudfront.net/datasets/rugged_data.csv\"\n",
    "# data = pd.read_csv(DATA_URL, encoding=\"ISO-8859-1\")\n",
    "# df = data[[\"cont_africa\", \"rugged\", \"rgdppc_2000\"]]\n",
    "# df = df[np.isfinite(df.rgdppc_2000)]\n",
    "# df[\"rgdppc_2000\"] = np.log(df[\"rgdppc_2000\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = torch.tensor(df.values, dtype=torch.float)\n",
    "# x_data, y_data = data[:, :-1], data[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def model(x_data, y_data):\n",
    "#     n = len(x_data)\n",
    "\n",
    "#     # w, b, sigma parameter is outside of plate, independent of N\n",
    "#     weight = pyro.sample(\"w\", dist.Normal(torch.zeros(1, 2), torch.ones(1, 2)))\n",
    "#     bias = pyro.sample(\"b\", dist.Normal(torch.tensor([[0.]]), torch.tensor([[100.]])))\n",
    "#     sigma = pyro.sample(\"epsilon\", Uniform(0., 10.))\n",
    "\n",
    "#     with pyro.plate(\"map\", n):\n",
    "#         mu = (x_data[:, 0] * weight[0][0] + x_data[:, 1] * weight[0][1] + bias).squeeze(1)\n",
    "#         yhat = pyro.sample(\"yhat\", Normal(mu, sigma), obs=y_data)\n",
    "#         return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Variational Inference with spherical gaussian on linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyro.contrib.autoguide import AutoDiagonalNormal\n",
    "# mean_field_guide = AutoDiagonalNormal(model)\n",
    "\n",
    "# # inject callables into SVI instantiation\n",
    "# svi = SVI(model, mean_field_guide, Adam({\"lr\": 0.03}), loss=Trace_ELBO(), num_samples=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.clear_param_store()\n",
    "# for j in range(2000):\n",
    "#     loss = svi.step(x_data, y_data)\n",
    "#     if j % 500 == 0:\n",
    "#         print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss / len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, value in pyro.get_param_store().items():\n",
    "#     print(name, pyro.param(name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No-U-turn Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.clear_param_store()\n",
    "\n",
    "# nuts = NUTS(model)\n",
    "# sampler = MCMC(nuts,\n",
    "#                num_samples=2000,\n",
    "#                num_chains=1,\n",
    "#                # burn-in\n",
    "#                warmup_steps=100)\n",
    "# traces = sampler.run(x_data, y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# posteriors = traces.marginal([\"w\", \"b\", \"epsilon\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = posteriors.empirical[\"epsilon\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_marginal(traces, sites):\n",
    "#     return EmpiricalMarginal(traces, sites)._get_samples_and_weights()[0].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# posterior_weight = posteriors.empirical[\"w\"]\n",
    "# posterior_bias = posteriors.empirical[\"b\"]\n",
    "# posterior_epsilon = posteriors.empirical[\"epsilon\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.distplot(posterior_weight((10000, )).squeeze(1)[:, 0])\n",
    "# sns.distplot(posterior_weight((10000, )).squeeze(1)[:, 1])\n",
    "# sns.distplot(posterior_bias((10000, )))\n",
    "# sns.distplot(posterior_epsilon((10000, )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vanila HMM before going BCRF (Qi et al. 2005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adopted from HMM tutorials at: https://pyro.ai/examples/hmm.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pyro's poutine handles effects \n",
    "from pyro import poutine\n",
    "import dmm.polyphonic_data_loader as poly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockArgs():\n",
    "    num_steps=500\n",
    "    hidden_dim=16\n",
    "    nn_dim=48\n",
    "    batch_size=32\n",
    "    nn_channels=2\n",
    "    learning_rate=0.05\n",
    "    truncate=None\n",
    "    print_shapes=False\n",
    "    jit=True\n",
    "    cuda=True\n",
    "    raftery_parameterization=True\n",
    "args = MockArgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.cuda:\n",
    "    torch.set_default_tensor_type('torch.cuda.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSB Chorales dataset, pianos keys pressed out of 88 keys.\n",
    "# Modeling Temporal Dependencies in High-Dimensional Sequences: Application to Polyphonic Music Generation and Transcription\n",
    "# http://www-etud.iro.umontreal.ca/~boulanni/icml2012\n",
    "data = poly.load_data(poly.JSB_CHORALES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = data['train']['sequences']\n",
    "lengths = data['train']['sequence_lengths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only some keys of this piano were pressed, some not, so reduce dimension of the sequence\n",
    "notes_pressed = ((sequences == 1).sum(0).sum(0) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = sequences[:, :, notes_pressed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.truncate:\n",
    "    lengths.clamp_(max=args.truncate)\n",
    "    sequences = sequences[:, :args.truncate]\n",
    "num_observations = float(lengths.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(sequences, lengths, args, batch_size=None, include_prior=True):\n",
    "    # Sometimes it is safe to ignore jit warnings. Here we use the\n",
    "    # pyro.util.ignore_jit_warnings context manager to silence warnings about\n",
    "    # conversion to integer, since we know all three numbers will be the same\n",
    "    # across all invocations to the model.\n",
    "    with ignore_jit_warnings():\n",
    "        num_sequences, max_length, data_dim = map(int, sequences.shape)\n",
    "        assert lengths.shape == (num_sequences, )\n",
    "        assert lengths.max() <= max_length\n",
    "\n",
    "    with poutine.mask(mask=include_prior):\n",
    "        # to_event seperates n right-most dimension as event dimension from batch dimension.\n",
    "        # transition probability p(y_t | y_t-1)\n",
    "        probs_x = pyro.sample(\"probs_x\", dist.Dirichlet(0.9 * torch.eye(args.hidden_dim) + 0.1).to_event(1))\n",
    "        # emission probability p(x_t | y_t)\n",
    "        probs_y = pyro.sample(\"probs_y\", dist.Beta(0.1, 0.9).expand([args.hidden_dim, data_dim]).to_event(2))\n",
    "\n",
    "    # nodes_plate in shape [DATA_DIM], in this case 51 dimension of each observation\n",
    "    nodes_plate = pyro.plate(\"nodes\", data_dim, dim=-1)\n",
    "    # We subsample batch_size items out of num_sequences items. Note that since\n",
    "    # we're using dim=-1 for the notes plate, we need to batch over a different\n",
    "    # dimension, here dim=-2.\n",
    "\n",
    "    with pyro.plate(\"sequences\", size=num_sequences, subsample_size=batch_size, dim=-2) as batch:\n",
    "        # here batch is indice of subsampled, in shape [BATCH_SIZE, 1].\n",
    "        batch_lengths = lengths[batch]\n",
    "        x = 0\n",
    "        for t in pyro.markov(range(max_length if args.jit else batch_lengths.max())):\n",
    "            with poutine.mask(mask=(t < batch_lengths).unsqueeze(-1)):\n",
    "                hidden_states = probs_x[x]\n",
    "                x = pyro.sample(\"x_{}\".format(t), dist.Categorical(hidden_states), infer={\"enumerate\": \"parallel\"})\n",
    "                # x is sampled from categorical distribution of [0, 1, 2, ..., hidden_dim], in batch\n",
    "                # x is the hidden states in shape [BATCH_SIZE] at markov process at time t, of latent variable probs_x\n",
    "                # nodes_plate is plate of N := data_dim\n",
    "                with nodes_plate:\n",
    "                    # y in size [BATCH_SIZE, DATA_DIM], generated by hidden_states at time t, which is x.\n",
    "                    probs_y_given_hidden_state = probs_y[x.squeeze(-1)]\n",
    "                    # bernoulli distribution because the music tones of data_dim 51 dimension is binary\n",
    "                    y = pyro.sample(\"y_{}\".format(t), dist.Bernoulli(probs_y_given_hidden_state),\n",
    "                                    # observed these y at t\n",
    "                                    obs=sequences[batch, t])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.contrib.autoguide import AutoDelta, AutoDiagonalNormal\n",
    "# Delta distribution for constrained MAP inference\n",
    "guide = AutoDelta(poutine.block(model, expose_fn=lambda msg: msg[\"name\"].startswith(\"probs_\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elbo = JitTraceEnum_ELBO(max_plate_nesting=2, strict_enumeration_warning=True)\n",
    "optim = Adam({'lr': 1e-3})\n",
    "svi = SVI(model, guide, optim, elbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.clear_param_store()\n",
    "# for step in range(args.num_steps * 10):\n",
    "#     loss = svi.step(sequences, lengths, args=args, batch_size=args.batch_size, include_prior=True)\n",
    "#     if not step % 50:\n",
    "#         print('{: >5d}\\t{}'.format(step, loss / num_observations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Turk data 20190716"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from pathlib import Path\n",
    "from typing import Union, Tuple, Iterable\n",
    "from smart_open import open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_bioes_parser(path: Union[Path, str]) -> Tuple:\n",
    "    \"\"\"simple parser for BIOES to BIO and remove puntuations\"\"\"\n",
    "    document_container = []\n",
    "    sequence_container = []\n",
    "    length_container = []\n",
    "\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            length = len(sequence_container)\n",
    "            line = line.rstrip()\n",
    "\n",
    "            # naively detect sentence boundary\n",
    "            if len(line) < 2:\n",
    "                if length > 0:\n",
    "                    length_container.append(length)\n",
    "                    document_container.append(list(zip(*sequence_container)))\n",
    "                    sequence_container = []\n",
    "                    continue\n",
    "\n",
    "            try:\n",
    "                word, entity_type = line.split('\\t')\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "            if word in string.punctuation:\n",
    "                continue\n",
    "\n",
    "            # skip lemmatization for later.\n",
    "            word = word.lower()\n",
    "\n",
    "            if \"-\" in entity_type:\n",
    "                a, b = entity_type.split(\"-\")\n",
    "                a = a.translate(str.maketrans(\"ES\", \"IB\"))\n",
    "                entity_type = \"-\".join([a, b])\n",
    "\n",
    "            sequence_container.append(tuple([word, entity_type]))\n",
    "\n",
    "        for d, l in zip(document_container, length_container):\n",
    "            tokens, ents = d\n",
    "            assert len(tokens) == l\n",
    "            assert len(ents) == l\n",
    "\n",
    "    return document_container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_documents = simple_bioes_parser(\"./data/turk_ner_20190716.txt\")\n",
    "raw_lengths = [len(d[0]) for d in raw_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "\n",
    "word_to_ix = {}\n",
    "tag_to_ix = {}\n",
    "\n",
    "# tag_to_ix[START_TAG] = len(tag_to_ix)\n",
    "\n",
    "for sentence, tags in raw_documents:\n",
    "    for word in sentence:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "    for tag in tags:\n",
    "        if tag not in tag_to_ix:\n",
    "            tag_to_ix[tag] = len(tag_to_ix)\n",
    "\n",
    "# tag_to_ix[STOP_TAG] = len(tag_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sents = [i[0] for i in raw_documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct = Dictionary(tokenized_sents)\n",
    "dct.filter_extremes()\n",
    "dct.compactify()\n",
    "# unknown token last in the vocabulary\n",
    "dct.token2id[\"UNK\"] = len(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dim = len(dct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockArgs():\n",
    "    data_dim = len(dct)\n",
    "    num_steps=1000\n",
    "    hidden_dim=len(tag_to_ix)\n",
    "    nn_dim=48\n",
    "    batch_size=32\n",
    "    nn_channels=2\n",
    "    learning_rate=0.05\n",
    "    truncate=200\n",
    "    print_shapes=False\n",
    "    jit=True\n",
    "    cuda=True\n",
    "    raftery_parameterization=True\n",
    "\n",
    "args = MockArgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorize_entities(seq, dictionary):\n",
    "    idxs = [tag_to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tensorize_sentence(seq):\n",
    "    idxs = dct.doc2idx(seq, unknown_word_index=0)\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sequences, tokenized_entities = list(zip(*raw_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_sents = list(map(tensorize_sentence, tokenized_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_ents = list(map(lambda x: tensorize_entities(x, tag_to_ix), tokenized_entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = torch.tensor([len(d[0]) for d in raw_documents], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(data: Iterable):\n",
    "    lengths = [d.shape[0] for d in data]\n",
    "    max_length = max(lengths)\n",
    "\n",
    "    template = torch.zeros(len(data), max_length, dtype=torch.long)\n",
    "    for k, tensor in enumerate(data):\n",
    "        template[k, :lengths[k]] = tensor\n",
    "\n",
    "    return template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BHMM(sequences, entities, lengths, args, include_prior=True):\n",
    "    with ignore_jit_warnings():\n",
    "        num_sequences = len(sequences)\n",
    "        max_length = max(lengths)\n",
    "        assert lengths.shape == (num_sequences, )\n",
    "        assert lengths.max() <= max_length\n",
    "\n",
    "    # to_event seperates n right-most dimension as event dimension from batch dimension.\n",
    "    with poutine.mask(mask=include_prior):\n",
    "        # transition probability p(y_t | y_t-1)\n",
    "        probs_x = pyro.sample(\"probs_x\", dist.Dirichlet(0.6 * torch.eye(args.hidden_dim) + 0.4).to_event(1))\n",
    "        # emission probability p(x_t | y_t)\n",
    "        probs_y = pyro.sample(\"probs_y\", dist.Dirichlet(torch.rand([args.hidden_dim, args.data_dim]) + 0.1).to_event(1))\n",
    "\n",
    "    with pyro.plate(\"sequences\", size=num_sequences, subsample_size=args.batch_size, dim=-2) as batch:\n",
    "        # here batch is indice of subsampled, in shape [BATCH_SIZE, 1].\n",
    "        batch_lengths = lengths[batch]\n",
    "\n",
    "        # start index of transition matrix for every sequence in the batch\n",
    "        x = 0\n",
    "        for t in pyro.markov(range(max_length)):\n",
    "            with poutine.mask(mask=(t < batch_lengths).unsqueeze(-1)):\n",
    "                hidden_states = probs_x[x]\n",
    "                x = pyro.sample(\"x_{}\".format(t),\n",
    "                                dist.Categorical(hidden_states),\n",
    "                                infer={\"enumerate\": \"parallel\"},\n",
    "                                obs=entities[batch, t].unsqueeze(-1))\n",
    "                # x is sampled from categorical distribution of [0, 1, 2, ..., tag_size], in batch\n",
    "                # x is the hidden states in shape [BATCH_SIZE] at markov process at time t, of latent variable probs_x\n",
    "                probs_y_given_st = probs_y[x]\n",
    "                # y in size [BATCH_SIZE, DATA_DIM], generated by hidden_states at time t, which is x.\n",
    "                # Categorical distribution for observable word generated from latent variable at z of certain value.\n",
    "                y = pyro.sample(\"y_{}\".format(t),\n",
    "                                dist.Categorical(probs_y_given_st),\n",
    "                                infer={\"enumerate\": \"parallel\"},\n",
    "                                obs=sequences[batch, t].unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guide = AutoDelta(poutine.block(BHMM, expose_fn=lambda msg: msg[\"name\"].startswith(\"probs_\")))\n",
    "elbo = Trace_ELBO(max_plate_nesting=2, strict_enumeration_warning=True)\n",
    "svi = SVI(BHMM, guide, Adam({'lr': 1e-4}), elbo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = pad_sequence(idx_sents)\n",
    "entities = pad_sequence(idx_ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.truncate:\n",
    "    lengths.clamp_(max=args.truncate)\n",
    "    sequences = sequences[:, :args.truncate]\n",
    "    entities = entities[:, :args.truncate]\n",
    "num_observations = float(lengths.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "\n",
    "nuts = NUTS(BHMM)\n",
    "sampler = MCMC(nuts,\n",
    "               num_samples=5000,\n",
    "               num_chains=1,\n",
    "               # burn-in\n",
    "               warmup_steps=200)\n",
    "traces = sampler.run(sequences, entities, lengths, args=args, include_prior=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyro.clear_param_store()\n",
    "# for step in range(args.num_steps * 10):\n",
    "#     loss = svi.step(sequences, entities, lengths, args=args, include_prior=True)\n",
    "#     if not step % 50:\n",
    "#         print('{: >5d}\\t{}'.format(step, loss / num_observations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_posterior(posterior):\n",
    "    # taken from \n",
    "    # generate Marginal distribution for `transition_prob` from posterior\n",
    "    marginal = posterior.marginal([\"transition_prob\"])\n",
    "    # get support of the marginal distribution\n",
    "    trace_transition_prob = marginal.support()[\"transition_prob\"]  # shape: num_samples x 3 x 3\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for i in range(num_categories):\n",
    "        for j in range(num_categories):\n",
    "            sns.distplot(trace_transition_prob[:, i, j], hist=False, kde_kws={\"lw\": 2},\n",
    "                         label=\"transition_prob[{}, {}], true value = {:.2f}\".format(i, j, transition_prob[i, j]))\n",
    "    plt.xlabel(\"Probability\", fontsize=13)\n",
    "    plt.ylabel(\"Frequency\", fontsize=13)\n",
    "    plt.title(\"Transition probability posterior\", fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_categories = 3\n",
    "num_words = 10\n",
    "num_supervised_data = 100\n",
    "num_data = 600\n",
    "\n",
    "transition_prior = torch.empty(num_categories).fill_(1.)\n",
    "emission_prior = torch.empty(num_words).fill_(0.1)\n",
    "\n",
    "transition_prob = dist.Dirichlet(transition_prior).sample(torch.Size([num_categories]))\n",
    "emission_prob = dist.Dirichlet(emission_prior).sample(torch.Size([num_categories]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def equilibrium(mc_matrix):\n",
    "    n = mc_matrix.size(0)\n",
    "    return (torch.eye(n) - mc_matrix.t() + 1).inverse().matmul(torch.ones(n))\n",
    "\n",
    "start_prob = equilibrium(transition_prob)\n",
    "\n",
    "# simulate data\n",
    "categories, words = [], []\n",
    "for t in range(num_data):\n",
    "    if t == 0 or t == num_supervised_data:\n",
    "        category = dist.Categorical(start_prob).sample()\n",
    "    else:\n",
    "        category = dist.Categorical(transition_prob[category]).sample()\n",
    "    word = dist.Categorical(emission_prob[category]).sample()\n",
    "    categories.append(category)\n",
    "    words.append(word)\n",
    "categories, words = torch.stack(categories), torch.stack(words)\n",
    "\n",
    "# split into supervised data and unsupervised data\n",
    "supervised_categories = categories[:num_supervised_data]\n",
    "supervised_words = categories[:num_supervised_data]\n",
    "unsupervised_words = categories[num_supervised_data:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervised_hmm(categories, words):\n",
    "    with pyro.plate(\"prob_plate\", num_categories):\n",
    "        transition_prob = pyro.sample(\"transition_prob\", dist.Dirichlet(transition_prior))\n",
    "        emission_prob = pyro.sample(\"emission_prob\", dist.Dirichlet(emission_prior))\n",
    "\n",
    "    category = categories[0]  # start with first category\n",
    "    for t in range(len(words)):\n",
    "        if t > 0:\n",
    "            category = pyro.sample(\"category_{}\".format(t), dist.Categorical(transition_prob[category]),\n",
    "                                   obs=categories[t])\n",
    "        pyro.sample(\"word_{}\".format(t), dist.Categorical(emission_prob[category]), obs=words[t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable jit_compile to improve the sampling speed\n",
    "nuts_kernel = NUTS(supervised_hmm, jit_compile=True, ignore_jit_warnings=True)\n",
    "mcmc = MCMC(nuts_kernel, num_samples=100)\n",
    "# we run MCMC to get posterior\n",
    "supervised_posterior = mcmc.run(supervised_categories, supervised_words)\n",
    "# after that, we plot the posterior\n",
    "plot_posterior(supervised_posterior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
